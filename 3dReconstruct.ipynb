{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiayu2002/test/blob/main/3dReconstruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_ySd7JQoruy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95dadac7-4b12-47a4-b545-3bdf3ca3e7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1r172cIGZKBc3b7_b1-cscPnVFj8bl8HF\n",
            "From (redirected): https://drive.google.com/uc?id=1r172cIGZKBc3b7_b1-cscPnVFj8bl8HF&confirm=t&uuid=d4d73112-c2d9-4223-bc70-a979b131582c\n",
            "To: /content/7SCENES.zip\n",
            "100%|██████████| 23.1G/23.1G [05:20<00:00, 71.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 已解壓縮到：./\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "file_id = \"1r172cIGZKBc3b7_b1-cscPnVFj8bl8HF\"\n",
        "zip_filename = \"7SCENES.zip\"\n",
        "extract_dir = \"./\"\n",
        "\n",
        "import gdown\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_filename, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"✅ 已解壓縮到：{extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow-heif\n",
        "!pip install open3d\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from typing import List,Dict,Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VHQdrDtBwkqY",
        "outputId": "62b965f9-cd8e-45ef-c0ef-9cbdad984876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow-heif\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow-heif) (11.2.1)\n",
            "Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m6.0/7.8 MB\u001b[0m \u001b[31m179.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m176.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow-heif\n",
            "Successfully installed pillow-heif-0.22.0\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.0.2)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.1)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.2.1)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting flask>=3.0.0 (from open3d)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting werkzeug>=3.0.0 (from open3d)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.4.26)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, widgetsnbextension, werkzeug, retrying, pyquaternion, jedi, configargparse, comm, flask, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 3.1.1\n",
            "    Uninstalling Flask-3.1.1:\n",
            "      Successfully uninstalled Flask-3.1.1\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7 dash-3.0.4 flask-3.0.3 ipywidgets-8.1.7 jedi-0.19.2 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate ground truth(seq2ply)\n",
        "INTRINSINC = (525, 525, 320, 240)  # fx, fy, cx, cy\n",
        "\n",
        "\n",
        "def imread_cv2(path:str, options=cv2.IMREAD_COLOR):\n",
        "    \"\"\"Open an image or a depthmap with opencv-python.\"\"\"\n",
        "    if path.endswith((\".exr\", \"EXR\")):\n",
        "        options = cv2.IMREAD_ANYDEPTH\n",
        "    img = cv2.imread(path, options)\n",
        "    if img is None:\n",
        "        raise IOError(f\"Could not load image={path} with {options=}\")\n",
        "    if img.ndim == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "def depthmap_to_world_coordinates(depthmap, camera_intrinsics, camera_pose, **kw):\n",
        "    \"\"\"\n",
        "    Projects a depth map into 3D world coordinates using camera intrinsics and optional pose.\n",
        "\n",
        "    Args:\n",
        "        depthmap (H x W): Depth values (in camera space).\n",
        "        intrinsics (3 x 3): Camera intrinsic matrix.\n",
        "        pose (optional, 4 x 4 or 4 x 3): Camera-to-world transformation.\n",
        "        pseudo_focal (optional, H x W): Per-pixel focal length override.\n",
        "\n",
        "    Returns:\n",
        "        pts_world (H x W x 3): 3D point cloud in world coordinates.\n",
        "        valid_mask (H x W): Boolean mask indicating valid (non-zero) depth pixels.\n",
        "    \"\"\"\n",
        "\n",
        "    H, W = depthmap.shape\n",
        "    camera_intrinsics = np.float32(camera_intrinsics)\n",
        "\n",
        "    # Extract intrinsic parameters\n",
        "    assert camera_intrinsics[0, 1] == 0.0 and camera_intrinsics[1, 0] == 0.0\n",
        "    fu,fv = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n",
        "    cu, cv = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n",
        "\n",
        "    # Generate pixel coordinate grid\n",
        "    u, v = np.meshgrid(np.arange(W), np.arange(H))  # u: cols, v: rows\n",
        "\n",
        "    # Backproject depth to 3D camera coordinates\n",
        "    z = depthmap\n",
        "    x = (u - cu) * z / fu\n",
        "    y = (v - cv) * z / fv\n",
        "    pts_cam = np.stack((x, y, z), axis=-1).astype(np.float32)\n",
        "\n",
        "    # Mark valid points (depth > 0)\n",
        "    valid_mask = z > 0.0\n",
        "\n",
        "    # Transform to world coordinates if pose is given\n",
        "    if camera_pose is not None:\n",
        "        R = camera_pose[:3, :3]\n",
        "        t = camera_pose[:3, 3]\n",
        "        pts_world = np.einsum(\"ik, vuk -> vui\", R, pts_cam) + t\n",
        "    else:\n",
        "        pts_world = pts_cam\n",
        "\n",
        "    return pts_world, valid_mask\n",
        "\n",
        "class SevenSceneSequence:\n",
        "    def __init__(\n",
        "            self,\n",
        "            seq_dir_path,\n",
        "        ):\n",
        "        self.seq_dir_path = seq_dir_path\n",
        "        # Find all the filenames end with \".color.png\"\n",
        "        # and check if corresponding \".proj.png\" and \".pose.txt\" exists\n",
        "\n",
        "        _color_files = [f for f in os.listdir(seq_dir_path) if f.endswith(\".color.png\")]\n",
        "        frame_names = [f.rstrip(\".color.png\") for f in _color_files]\n",
        "\n",
        "        self.valid_frame_names = []\n",
        "        for name in frame_names:\n",
        "            proj_path = osp.join(seq_dir_path, f\"{name}.depth.proj.png\")\n",
        "            pose_path = osp.join(seq_dir_path, f\"{name}.pose.txt\")\n",
        "            if osp.isfile(proj_path) and osp.isfile(pose_path):\n",
        "                self.valid_frame_names.append(name)\n",
        "        self.valid_frame_names = sorted(self.valid_frame_names)\n",
        "\n",
        "        print(f\"{len(self.valid_frame_names)} frames collected in {self.seq_dir_path}!!\")\n",
        "        print(f\"{len(_color_files) - len(self.valid_frame_names)} rgb frames miss .proj.png or .pose.txt!!\")\n",
        "\n",
        "\n",
        "    def get_views(self,kf_every = 200)->List[Dict]:\n",
        "\n",
        "        names = self.valid_frame_names[::kf_every] # select 1 out of every kf_every frames for reconstruction\n",
        "\n",
        "        views = []\n",
        "        \"\"\"\n",
        "        For each view(key frame), we compute the following metric\n",
        "        \"\"\"\n",
        "        for idx,name in enumerate(names):\n",
        "            view = dict()\n",
        "\n",
        "            impath = osp.join(self.seq_dir_path, f\"{name}.color.png\")\n",
        "            depthpath = osp.join(self.seq_dir_path, f\"{name}.depth.proj.png\")\n",
        "            posepath = osp.join(self.seq_dir_path, f\"{name}.pose.txt\")\n",
        "            view[\"name\"] = f'{self.seq_dir_path}/{name}'\n",
        "\n",
        "            rgb_image = imread_cv2(impath)\n",
        "            depthmap = imread_cv2(depthpath, cv2.IMREAD_UNCHANGED)\n",
        "            rgb_image = cv2.resize(rgb_image, (depthmap.shape[1], depthmap.shape[0]))\n",
        "\n",
        "            width, height = Image.fromarray(rgb_image).size\n",
        "            assert (width,height) == (640,480)\n",
        "            view['img'] = (rgb_image / 255.0 ).astype(np.float32)# Normalize to 0 to 1 for open3d format\n",
        "            view[\"true_shape\"] = np.int32((height, width))\n",
        "\n",
        "            depthmap[depthmap == 65535] = 0\n",
        "            depthmap = np.nan_to_num(depthmap.astype(np.float32), 0.0) / 1000.0\n",
        "            depthmap[depthmap > 10] = 0\n",
        "            depthmap[depthmap < 1e-3] = 0\n",
        "            assert np.isfinite(depthmap).all(), \\\n",
        "                f\"NaN in depthmap for view {view['name']}\"\n",
        "            view['depthmap'] = depthmap\n",
        "\n",
        "            camera_pose = np.loadtxt(posepath).astype(np.float32)\n",
        "            fx, fy, cx, cy = INTRINSINC ### NOTE: This intrinsic does not match with that on internet\n",
        "            intrinsics = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)\n",
        "            assert np.isfinite(camera_pose).all(), \\\n",
        "                f\"NaN in camera pose for view {view['name']}\"\n",
        "\n",
        "            view['camera_pose'] = camera_pose\n",
        "            view['camera_intrinsics'] = intrinsics\n",
        "\n",
        "            # encode the image\n",
        "            pts3d, valid_mask = depthmap_to_world_coordinates(**view)\n",
        "            view[\"pts3d\"] = pts3d\n",
        "            view[\"valid_mask\"] = valid_mask & np.isfinite(pts3d).all(axis=-1)\n",
        "            view[\"img_mask\"] = True\n",
        "\n",
        "            # check all datatypes\n",
        "            for key, val in view.items():\n",
        "                res, err_msg = self._is_good_type(key, val)\n",
        "                assert res, f\"{err_msg} with {key}={val} for view {view['name']}\"\n",
        "\n",
        "            views.append(view)\n",
        "\n",
        "        for view in views:\n",
        "            height, width = view['true_shape']\n",
        "            assert width >= height, ValueError(\"Width > Height\")\n",
        "\n",
        "        return views\n",
        "\n",
        "    def _is_good_type(self,key, v):\n",
        "        \"\"\"returns (is_good, err_msg)\"\"\"\n",
        "        if isinstance(v, (str, int, tuple)):\n",
        "            return True, None\n",
        "        if v.dtype not in (np.float32, bool, np.int32, np.int64, np.uint8):\n",
        "            return False, f\"bad {v.dtype=}\"\n",
        "        return True, None\n",
        "\n",
        "def seq2ply(seq_dir_path, ply_path, kf_every = 1, crop_size = None, voxel_grid_size = None):\n",
        "    \"\"\"\n",
        "    Converts a sequence of frames into a single 3D point cloud and saves it as a .ply file.\n",
        "\n",
        "    Parameters:\n",
        "        seq_dir_path (str): Path to the sequence directory. This directory should contain multiple\n",
        "                            frame subdirectories or files, each including:\n",
        "                                - .color.png: RGB image\n",
        "                                - .proj.png: Projected depth or coordinate image\n",
        "                                - .pose.txt: Camera pose matrix (usually 4x4)\n",
        "\n",
        "        ply_path (str): Destination path for the output .ply point cloud file.\n",
        "        kf_every (int): Selec key frame every \"kf_every\" frames for building points cloud\n",
        "\n",
        "    Description:\n",
        "        This function reads all frames in the given sequence directory, reconstructs 3D points using the color,\n",
        "        projection, and pose data, merges them into a single point cloud, and writes the result to\n",
        "        a .ply file.\n",
        "    \"\"\"\n",
        "    # Step 1: Collect the necessary information of frames for reconstruction\n",
        "    seq = SevenSceneSequence(seq_dir_path = seq_dir_path )\n",
        "    views = seq.get_views(kf_every = kf_every)\n",
        "    pts_gt_all, images_all,  masks_all = [], [], []\n",
        "\n",
        "    # Step 2: Only believe the central information of the camera\n",
        "    assert crop_size is None \\\n",
        "        or isinstance(crop_size, int), \\\n",
        "        \"crop_size must be None or an integer\"\n",
        "\n",
        "    for _, view in enumerate(views):\n",
        "        image = view[\"img\"]  # W,H,3\n",
        "        mask = view[\"valid_mask\"]    # W,H\n",
        "        pts_gt = view['pts3d'] # W,H,3\n",
        "\n",
        "        # Center on the given window size\n",
        "        if crop_size is not None:\n",
        "            H, W = image.shape[:2]\n",
        "            if crop_size > H or crop_size > W:\n",
        "                print(f\"Warning: Adjust crop_size({crop_size}) since it exceeds H({H}) or W({W})\")\n",
        "                crop_size = min(W,H)\n",
        "            _shift = crop_size//2\n",
        "            cx,cy = W // 2,H // 2\n",
        "            l, t = cx - _shift, cy - _shift # left, top\n",
        "            r, b = cx + _shift, cy + _shift # right, bottom\n",
        "\n",
        "            image = image[t:b, l:r]\n",
        "            mask = mask[t:b, l:r]\n",
        "            pts_gt = pts_gt[t:b, l:r]\n",
        "\n",
        "        #### Align predicted 3D points to the ground truth\n",
        "        images_all.append( image[None, ...] )\n",
        "        pts_gt_all.append( pts_gt[None, ...] )\n",
        "        masks_all.append( mask[None, ...] )\n",
        "\n",
        "\n",
        "    # Step 3: Build the 3D points map\n",
        "    images_all = np.concatenate(images_all, axis=0)\n",
        "    pts_gt_all = np.concatenate(pts_gt_all, axis=0)\n",
        "    masks_all = np.concatenate(masks_all, axis=0)\n",
        "    pts_gt_all_masked = pts_gt_all[masks_all > 0]\n",
        "    images_all_masked = images_all[masks_all > 0]\n",
        "\n",
        "    #save_params = {}\n",
        "    #save_params[\"images_all\"] = images_all\n",
        "    #save_params[\"pts_gt_all\"] = pts_gt_all\n",
        "    #save_params[\"masks_all\"] = masks_all\n",
        "    #np.save(_path_,save_params,)\n",
        "\n",
        "    pcd_gt = o3d.geometry.PointCloud()\n",
        "    pcd_gt.points = o3d.utility.Vector3dVector(\n",
        "        pts_gt_all_masked.reshape(-1, 3)\n",
        "    )\n",
        "    pcd_gt.colors = o3d.utility.Vector3dVector(\n",
        "        images_all_masked.reshape(-1, 3)\n",
        "    )\n",
        "    print(f'Points Cloud has {len(pcd_gt.points)} points')\n",
        "    if voxel_grid_size is not None:\n",
        "        pcd_gt = pcd_gt.voxel_down_sample(voxel_size=voxel_grid_size)\n",
        "        print(f'After downsample, Points Cloud has {len(pcd_gt.points)} points')\n",
        "\n",
        "    o3d.io.write_point_cloud(ply_path, pcd_gt, )"
      ],
      "metadata": {
        "id": "8RLpxYFgmcRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ground_truth_ply(scenes_root, pointcloud_root, scene_list, split='train',\n",
        "                               kf_every=20, voxel_grid_size=0.0075, enable=True):\n",
        "    \"\"\"\n",
        "    產生 ground truth 點雲並儲存成 .ply 檔案。\n",
        "\n",
        "    Args:\n",
        "        scenes_root (str): 圖像的根目錄 (ex: '../7SCENES')\n",
        "        pointcloud_root (str): 儲存 .ply 的資料夾 (ex: './train_truth')\n",
        "        scene_list (List[str]): 需要處理的 scene 名稱列表\n",
        "        split (str): 'train' 或 'test'\n",
        "        kf_every (int): 每隔幾張 frame 選一次 keyframe\n",
        "        voxel_grid_size (float): voxel downsample 的大小\n",
        "        enable (bool): 若為 False，將完全跳過處理\n",
        "    \"\"\"\n",
        "    if not enable:\n",
        "        print(f\"[INFO] Ground truth generation for split '{split}' is disabled.\")\n",
        "        return\n",
        "    os.makedirs(pointcloud_root, exist_ok=True)\n",
        "\n",
        "    for scene in scene_list:\n",
        "        root_path = osp.join(scenes_root, scene)\n",
        "        split_path = osp.join(root_path, split)\n",
        "        split_txt = osp.join(root_path, f\"{split.capitalize()}Split.txt\")\n",
        "\n",
        "        if not osp.isfile(split_txt):\n",
        "            print(f\"[WARNING] {split_txt} 不存在，跳過 scene {scene}\")\n",
        "            continue\n",
        "\n",
        "        with open(split_txt, \"r\") as f:\n",
        "            seq_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        for seq in seq_names:\n",
        "            seq_num = int(seq.replace(\"sequence\", \"\"))\n",
        "            seq_dir = osp.join(split_path, f\"seq-{seq_num:02d}\")\n",
        "            if not osp.isdir(seq_dir):\n",
        "                print(f\"[WARNING] 資料夾不存在：{seq_dir}，跳過\")\n",
        "                continue\n",
        "\n",
        "            ply_path = osp.join(pointcloud_root, f\"{scene}-seq-{seq_num}.ply\")\n",
        "            if osp.isfile(ply_path):\n",
        "                print(f\"[SKIP] 已存在：{ply_path}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"[INFO] 正在處理：{scene} - seq-{seq_num:02d}\")\n",
        "            seq2ply(seq_dir, ply_path, kf_every=kf_every, voxel_grid_size=voxel_grid_size)"
      ],
      "metadata": {
        "id": "Xz4kbKpNqecX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as tvf\n",
        "import PIL.Image\n",
        "from pillow_heif import register_heif_opener\n",
        "import re\n",
        "from PIL import ExifTags\n",
        "def exif_transpose(image: Image.Image, *, in_place: bool = False) -> Image.Image | None:\n",
        "    \"\"\"\n",
        "    If an image has an EXIF Orientation tag, other than 1, transpose the image\n",
        "    accordingly, and remove the orientation data.\n",
        "\n",
        "    :param image: The image to transpose.\n",
        "    :param in_place: Boolean. Keyword-only argument.\n",
        "        If ``True``, the original image is modified in-place, and ``None`` is returned.\n",
        "        If ``False`` (default), a new :py:class:`~PIL.Image.Image` object is returned\n",
        "        with the transposition applied. If there is no transposition, a copy of the\n",
        "        image will be returned.\n",
        "    \"\"\"\n",
        "    image.load()\n",
        "    image_exif = image.getexif()\n",
        "    orientation = image_exif.get(ExifTags.Base.Orientation, 1)\n",
        "    method = {\n",
        "        2: Image.Transpose.FLIP_LEFT_RIGHT,\n",
        "        3: Image.Transpose.ROTATE_180,\n",
        "        4: Image.Transpose.FLIP_TOP_BOTTOM,\n",
        "        5: Image.Transpose.TRANSPOSE,\n",
        "        6: Image.Transpose.ROTATE_270,\n",
        "        7: Image.Transpose.TRANSVERSE,\n",
        "        8: Image.Transpose.ROTATE_90,\n",
        "    }.get(orientation)\n",
        "    if method is not None:\n",
        "        if in_place:\n",
        "            image.im = image.im.transpose(method)\n",
        "            image._size = image.im.size\n",
        "        else:\n",
        "            transposed_image = image.transpose(method)\n",
        "        exif_image = image if in_place else transposed_image\n",
        "\n",
        "        exif = exif_image.getexif()\n",
        "        if ExifTags.Base.Orientation in exif:\n",
        "            del exif[ExifTags.Base.Orientation]\n",
        "            if \"exif\" in exif_image.info:\n",
        "                exif_image.info[\"exif\"] = exif.tobytes()\n",
        "            elif \"Raw profile type exif\" in exif_image.info:\n",
        "                exif_image.info[\"Raw profile type exif\"] = exif.tobytes().hex()\n",
        "            for key in (\"XML:com.adobe.xmp\", \"xmp\"):\n",
        "                if key in exif_image.info:\n",
        "                    for pattern in (\n",
        "                        r'tiff:Orientation=\"([0-9])\"',\n",
        "                        r\"<tiff:Orientation>([0-9])</tiff:Orientation>\",\n",
        "                    ):\n",
        "                        value = exif_image.info[key]\n",
        "                        exif_image.info[key] = (\n",
        "                            re.sub(pattern, \"\", value)\n",
        "                            if isinstance(value, str)\n",
        "                            else re.sub(pattern.encode(), b\"\", value)\n",
        "                        )\n",
        "        if not in_place:\n",
        "            return transposed_image\n",
        "    elif not in_place:\n",
        "        return image.copy()\n",
        "    return None\n",
        "def _resize_pil_image(img, long_edge_size):\n",
        "    S = max(img.size)\n",
        "    if S > long_edge_size:\n",
        "        interp = PIL.Image.LANCZOS\n",
        "    elif S <= long_edge_size:\n",
        "        interp = PIL.Image.BICUBIC\n",
        "    new_size = tuple(int(round(x * long_edge_size / S)) for x in img.size)\n",
        "    return img.resize(new_size, interp)\n",
        "\n",
        "def load_images(folder_or_list, size, square_ok=False, verbose=True, rotate_clockwise_90=False, crop_to_landscape=False):\n",
        "    \"\"\"open and convert all images in a list or folder to proper input format for DUSt3R\"\"\"\n",
        "    ImgNorm = tvf.Compose([tvf.ToTensor(), tvf.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    try:\n",
        "        register_heif_opener()\n",
        "        heif_support_enabled = True\n",
        "    except ImportError:\n",
        "        heif_support_enabled = False\n",
        "\n",
        "    if isinstance(folder_or_list, str):\n",
        "        if verbose:\n",
        "            print(f\">> Loading images from {folder_or_list}\")\n",
        "        root, folder_content = folder_or_list, sorted(os.listdir(folder_or_list))\n",
        "\n",
        "    elif isinstance(folder_or_list, list):\n",
        "        if verbose:\n",
        "            print(f\">> Loading a list of {len(folder_or_list)} images\")\n",
        "        root, folder_content = \"\", folder_or_list\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"bad {folder_or_list=} ({type(folder_or_list)})\")\n",
        "\n",
        "    supported_images_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
        "    if heif_support_enabled:\n",
        "        supported_images_extensions += [\".heic\", \".heif\"]\n",
        "    supported_images_extensions = tuple(supported_images_extensions)\n",
        "\n",
        "    imgs = []\n",
        "    for path in folder_content:\n",
        "        if not path.lower().endswith(supported_images_extensions):\n",
        "            continue\n",
        "        img = exif_transpose(PIL.Image.open(os.path.join(root, path))).convert(\"RGB\")\n",
        "        if rotate_clockwise_90:\n",
        "            img = img.rotate(-90, expand=True)\n",
        "        if crop_to_landscape:\n",
        "            # Crop to a landscape aspect ratio (e.g., 16:9)\n",
        "            desired_aspect_ratio = 4 / 3\n",
        "            width, height = img.size\n",
        "            current_aspect_ratio = width / height\n",
        "\n",
        "            if current_aspect_ratio > desired_aspect_ratio:\n",
        "                # Wider than landscape: crop width\n",
        "                new_width = int(height * desired_aspect_ratio)\n",
        "                left = (width - new_width) // 2\n",
        "                right = left + new_width\n",
        "                top = 0\n",
        "                bottom = height\n",
        "            else:\n",
        "                # Taller than landscape: crop height\n",
        "                new_height = int(width / desired_aspect_ratio)\n",
        "                top = (height - new_height) // 2\n",
        "                bottom = top + new_height\n",
        "                left = 0\n",
        "                right = width\n",
        "\n",
        "            img = img.crop((left, top, right, bottom))\n",
        "\n",
        "        W1, H1 = img.size\n",
        "        if size == 224:\n",
        "            # resize short side to 224 (then crop)\n",
        "            img = _resize_pil_image(img, round(size * max(W1 / H1, H1 / W1)))\n",
        "        else:\n",
        "            # resize long side to 512\n",
        "            img = _resize_pil_image(img, size)\n",
        "        W, H = img.size\n",
        "        cx, cy = W // 2, H // 2\n",
        "        if size == 224:\n",
        "            half = min(cx, cy)\n",
        "            img = img.crop((cx - half, cy - half, cx + half, cy + half))\n",
        "        else:\n",
        "            halfw, halfh = ((2 * cx) // 16) * 8, ((2 * cy) // 16) * 8\n",
        "            if not (square_ok) and W == H:\n",
        "                halfh = 3 * halfw / 4\n",
        "            img = img.crop((cx - halfw, cy - halfh, cx + halfw, cy + halfh))\n",
        "\n",
        "        W2, H2 = img.size\n",
        "        if verbose:\n",
        "            print(f\" - adding {path} with resolution {W1}x{H1} --> {W2}x{H2}\")\n",
        "        imgs.append(\n",
        "            dict(\n",
        "                img=ImgNorm(img)[None],\n",
        "                true_shape=np.int32([img.size[::-1]]),\n",
        "                idx=len(imgs),\n",
        "                instance=str(len(imgs)),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    assert imgs, \"no images foud at \" + root\n",
        "    if verbose:\n",
        "        print(f\" (Found {len(imgs)} images)\")\n",
        "    return imgs"
      ],
      "metadata": {
        "id": "9qZTCXPCoMMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate train/test dataset\n",
        "def get_keyframe_paths(seq_dir, kf_every=20):\n",
        "    image_paths = []\n",
        "    for fname in sorted(os.listdir(seq_dir)):\n",
        "        if fname.endswith(\".color.png\"):\n",
        "            frame_id = int(fname.replace(\"frame-\", \"\").replace(\".color.png\", \"\"))\n",
        "            if frame_id % kf_every == 0:\n",
        "                image_paths.append(os.path.join(seq_dir, fname)\n",
        "                )\n",
        "\n",
        "    return image_paths\n",
        "def load_depth_cv2(path, size, square_ok=False):\n",
        "    depth = cv2.imread(path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
        "    depth[depth == 65535] = np.nan\n",
        "    depth /= 1000.0\n",
        "    depth[(depth < 1e-3) | (depth > 10.0)] = np.nan\n",
        "\n",
        "    H1, W1 = depth.shape\n",
        "\n",
        "    # Resize：與 load_images 對齊\n",
        "    if size == 224:\n",
        "        # resize short side to 224\n",
        "        scale = round(size * max(W1 / H1, H1 / W1)) / max(W1, H1)\n",
        "        new_size = (int(W1 * scale), int(H1 * scale))\n",
        "    else:\n",
        "        # resize long side to size\n",
        "        scale = size / max(W1, H1)\n",
        "        new_size = (int(W1 * scale), int(H1 * scale))\n",
        "\n",
        "    depth = cv2.resize(depth, new_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Center crop\n",
        "    H, W = depth.shape\n",
        "    cx, cy = W // 2, H // 2\n",
        "    if size == 224:\n",
        "        half = min(cx, cy)\n",
        "        depth = depth[cy - half:cy + half, cx - half:cx + half]\n",
        "    else:\n",
        "        halfw = ((2 * cx) // 16) * 8\n",
        "        halfh = ((2 * cy) // 16) * 8\n",
        "        if not square_ok and W == H:\n",
        "            halfh = int(3 * halfw / 4)\n",
        "        depth = depth[cy - halfh:cy + halfh, cx - halfw:cx + halfw]\n",
        "\n",
        "\n",
        "    mask = (~np.isnan(depth)).astype(np.float32)\n",
        "    depth = np.nan_to_num(depth, nan=0.0)\n",
        "\n",
        "    return depth, mask\n",
        "\n",
        "class MultiViewPointCloudDataset(Dataset):\n",
        "    def __init__(self, scenes_root, pointcloud_root, scene_list, kf_every=20, views_per_sample=5, size=384, split='train'):\n",
        "        \"\"\"\n",
        "        scenes_root: 路径，如 '../7SCENES'\n",
        "        pointcloud_root: 对应点云的路径，如 './test_truth'\n",
        "        scene_list: ['chess', 'fire', ...]\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'test'], \"split must be 'train' or 'test'\"\n",
        "        self.samples = []\n",
        "        self.size = size\n",
        "        self.views_per_sample = views_per_sample\n",
        "\n",
        "        for scene in scene_list:\n",
        "            scene_path = osp.join(scenes_root, scene)\n",
        "            split_dir = osp.join(scene_path, split)\n",
        "            split_txt = osp.join(scene_path, f'{split.capitalize()}Split.txt')\n",
        "\n",
        "            with open(split_txt, \"r\") as f:\n",
        "                seq_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "            for seq in seq_names:\n",
        "                seq_num = int(seq.replace(\"sequence\", \"\"))\n",
        "                seq_dir = osp.join(split_dir, f\"seq-{seq_num:02d}\")\n",
        "                ply_path = osp.join(pointcloud_root, f\"{scene}-seq-{seq_num}.ply\")\n",
        "\n",
        "                if not osp.isdir(seq_dir) :\n",
        "                    continue\n",
        "\n",
        "                image_paths = get_keyframe_paths(seq_dir, kf_every=kf_every)\n",
        "                if len(image_paths) < views_per_sample:\n",
        "                    continue\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"image_paths\": image_paths[:-1],\n",
        "                    \"ply_path\": ply_path\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image_paths = sample[\"image_paths\"]\n",
        "        ply_path = sample[\"ply_path\"]\n",
        "        depth_paths = [path.replace(\"color.png\", \"depth.proj.png\") for path in image_paths]\n",
        "        depthmaps = []\n",
        "        valid_masks = []\n",
        "        for path in depth_paths:\n",
        "\n",
        "            depth, mask = load_depth_cv2(path, self.size)\n",
        "            depthmaps.append(torch.from_numpy(depth[None]))     # [1, H, W]\n",
        "            valid_masks.append(torch.from_numpy(mask[None]))     # [1, H, W]\n",
        "\n",
        "        depths = torch.stack(depthmaps)       # [V, 1, H, W]\n",
        "        masks = torch.stack(valid_masks)      # [V, 1, H, W]\n",
        "\n",
        "        images = load_images(image_paths, size=self.size, verbose=False)\n",
        "        images = torch.stack([img_dict[\"img\"].squeeze(0) for img_dict in images])\n",
        "        gt_pcd = o3d.io.read_point_cloud(ply_path)\n",
        "        gt_points = np.asarray(gt_pcd.points).astype(np.float32)  # (N, 3)\n",
        "\n",
        "        return {\n",
        "            \"images\": images,              # [V, 3, H, W] or list\n",
        "            \"depths\": depths,              # [V, 1, H, W]\n",
        "            \"masks\": masks,                # [V, 1, H, W]\n",
        "            \"target_pointcloud\": torch.from_numpy(gt_points),  # [N, 3]\n",
        "            \"image_paths\": image_paths\n",
        "        }"
      ],
      "metadata": {
        "id": "Tq3TnnkvvFZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "#model resnet+ace\n",
        "class ACEHead(nn.Module):\n",
        "    def __init__(self, in_channels=512, mid_channels=256):\n",
        "        super().__init__()\n",
        "        # dense block + skip block (parallel)\n",
        "        self.skip = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(mid_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(mid_channels, mid_channels, 1),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        self.eca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),                  # [B, C, 1, 1]\n",
        "            nn.Conv2d(mid_channels * 2, 1, kernel_size=1),  # 用 Conv2d 而不是 Conv1d\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Conv2d(mid_channels * 2, 4, 1)  # [x, y, z, w_hat]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.skip(x)\n",
        "        x2 = self.dense(x)\n",
        "        x_cat = torch.cat([x1, x2], dim=1)\n",
        "\n",
        "        # channel attention\n",
        "        attn = self.eca(x_cat).view(x_cat.shape[0], -1, 1, 1)\n",
        "        x_attn = x_cat * attn\n",
        "\n",
        "        return self.output_layer(x_attn)\n",
        "\n",
        "\n",
        "class ResNetBackbone(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_layer='layer4'):\n",
        "        super().__init__()\n",
        "        model = resnet18(pretrained=True)\n",
        "        model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        model.layer4[0].conv1.stride = (1, 1)\n",
        "        model.layer4[0].downsample[0].stride = (1, 1)\n",
        "\n",
        "        return_nodes = {out_layer: \"features\"}\n",
        "        self.backbone = create_feature_extractor(model, return_nodes=return_nodes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)[\"features\"]  # [B, 512, H', W']\n",
        "\n",
        "class RGBDPointPredictor(nn.Module):\n",
        "    def __init__(self, output_size=(288, 384)):\n",
        "        super().__init__()\n",
        "        self.rgb_backbone = ResNetBackbone(in_channels=3)\n",
        "        self.depth_backbone = ResNetBackbone(in_channels=1)\n",
        "        self.fusion_conv = nn.Conv2d(512 * 2, 512, kernel_size=1)  # fuse channel-wise\n",
        "        self.head = ACEHead(in_channels=512)\n",
        "        self.output_size = output_size  # (H, W)\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        \"\"\"\n",
        "        rgb: [B, 3, H, W]  (e.g., [B, 3, 288, 384])\n",
        "        depth: [B, 1, H, W]\n",
        "        \"\"\"\n",
        "        rgb_feat = self.rgb_backbone(rgb)      # [B, 512, H', W']\n",
        "        dpt_feat = self.depth_backbone(depth)  # [B, 512, H', W']\n",
        "        feat = torch.cat([rgb_feat, dpt_feat], dim=1)  # [B, 1024, H', W']\n",
        "        fused = self.fusion_conv(feat)                # [B, 512, H', W']\n",
        "        output = self.head(fused)                     # [B, 4, H', W']\n",
        "\n",
        "        output_upsampled = F.interpolate(\n",
        "            output,\n",
        "            size=self.output_size,\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )  # [B, 4, 288, 384]\n",
        "\n",
        "        return output_upsampled\n"
      ],
      "metadata": {
        "id": "lei3Q-Bvv2oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import cKDTree\n",
        "def chamfer_distance(p1, p2):\n",
        "    \"\"\"\n",
        "    Chamfer Distance between two point clouds without batch dim.\n",
        "\n",
        "    Args:\n",
        "        p1: Tensor (P1, D)\n",
        "        p2: Tensor (P2, D)\n",
        "\n",
        "    Returns:\n",
        "        scalar loss\n",
        "    \"\"\"\n",
        "    diff = p1.unsqueeze(1) - p2.unsqueeze(0)   # (P1, P2, D)\n",
        "    dist = torch.sum(diff ** 2, dim=-1)        # (P1, P2)\n",
        "\n",
        "    min_dist_p1, _ = torch.min(dist, dim=1)    # (P1,)\n",
        "    min_dist_p2, _ = torch.min(dist, dim=0)    # (P2,)\n",
        "\n",
        "    loss = min_dist_p1.mean() + min_dist_p2.mean()\n",
        "    return loss\n",
        "def point_cloud_accuracy(pred_points: np.ndarray, gt_points: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Median distance from each predicted point to its nearest ground truth point.\n",
        "    \"\"\"\n",
        "    tree = cKDTree(gt_points)\n",
        "    distances, _ = tree.query(pred_points, k=1)\n",
        "    return np.median(distances)\n",
        "\n",
        "def point_cloud_completeness(pred_points: np.ndarray, gt_points: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Median distance from each ground-truth point to its nearest predicted point.\n",
        "    \"\"\"\n",
        "    tree = cKDTree(pred_points)\n",
        "    distances, _ = tree.query(gt_points, k=1)\n",
        "    return np.median(distances)"
      ],
      "metadata": {
        "id": "nfANLEiHv996"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/test dependency\n",
        "def random_sampling(points, num_samples=2048):\n",
        "    if points.shape[0] > num_samples:\n",
        "        idx = torch.randperm(points.shape[0])[:num_samples]\n",
        "        return points[idx]\n",
        "    else:\n",
        "        return points\n",
        "def extract_scene_and_seq(path):\n",
        "    # path: './7SCENES/stairs/train/seq-06/frame-000000.color.png'\n",
        "    parts = path.split(os.sep)\n",
        "    scene = parts[-4]               # 'stairs'\n",
        "    sequence_id = int(parts[-2].split('-')[1])  # '06' → 6\n",
        "    return scene, sequence_id\n",
        "def evaluate(model, loader, device, desc=\"Evaluation\", save_results=False, save_dir=\"./test\"):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  total_acc = 0\n",
        "  total_comp = 0\n",
        "  with torch.no_grad():\n",
        "      for data in tqdm(loader, desc=desc, leave=False):\n",
        "          W = data[\"images\"].shape[-1]\n",
        "          H = data[\"images\"].shape[-2]\n",
        "          images = data[\"images\"].view(-1, 3, H, W).to(device)\n",
        "          depths = data[\"depths\"].view(-1, 1, H, W).to(device)\n",
        "          target_pcd = data[\"target_pointcloud\"].to(device)\n",
        "\n",
        "          pred = model(images, depths)\n",
        "          xyz = pred[:, :3].permute(0, 2, 3, 1).reshape(-1, 3)\n",
        "          xyz_sampled = random_sampling(xyz, 8192)\n",
        "          target_sampled = random_sampling(target_pcd[0], 8192)\n",
        "\n",
        "          loss = chamfer_distance(xyz_sampled, target_sampled)\n",
        "          acc = point_cloud_accuracy(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "          comp = point_cloud_completeness(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "\n",
        "          total_loss += loss.item()\n",
        "          total_acc += acc\n",
        "          total_comp += comp\n",
        "          # Save prediction as .ply file\n",
        "          if save_results:\n",
        "              scene, sequence_id = extract_scene_and_seq(data[\"image_paths\"][0][0])\n",
        "              save_path = os.path.join(save_dir, f\"{scene}-seq-{sequence_id:02d}.ply\")\n",
        "\n",
        "\n",
        "              pcd = o3d.geometry.PointCloud()\n",
        "              pcd.points = o3d.utility.Vector3dVector(xyz_sampled.detach().cpu().numpy())\n",
        "              o3d.io.write_point_cloud(save_path, pcd)\n",
        "\n",
        "  num_batches = len(loader)\n",
        "  return total_loss / num_batches, total_acc / num_batches, total_comp / num_batches"
      ],
      "metadata": {
        "id": "lUq50MLFrEYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "from torch.utils.data import random_split\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train(device):\n",
        "    dataset = MultiViewPointCloudDataset(\n",
        "        scenes_root=\"./7SCENES\",\n",
        "        pointcloud_root=\"./train_truth\",\n",
        "        scene_list = ['chess', 'fire', 'heads', 'office', 'pumpkin', 'redkitchen', 'stairs'],\n",
        "        kf_every=10,\n",
        "        views_per_sample=10, #total choose img number\n",
        "        size=384,\n",
        "        split='train'\n",
        "    )\n",
        "\n",
        "    val_size = int(0.3 * len(dataset))\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = RGBDPointPredictor().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "    save_dir = \"./checkpoints\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    num_epochs = 60\n",
        "    patience = 5  # early stop patience\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, total_acc, total_comp = 0, 0, 0\n",
        "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for data in progress:\n",
        "            W, H = data[\"images\"].shape[-1], data[\"images\"].shape[-2]\n",
        "            images = data[\"images\"].view(-1, 3, H, W).to(device)\n",
        "            depths = data[\"depths\"].view(-1, 1, H, W).to(device)\n",
        "            target_pcd = data[\"target_pointcloud\"].to(device)\n",
        "\n",
        "            pred = model(images, depths)\n",
        "            xyz = pred[:, :3].permute(0, 2, 3, 1).reshape(-1, 3)\n",
        "            xyz_sampled = random_sampling(xyz, 2048)\n",
        "            target_sampled = random_sampling(target_pcd[0], 8192)\n",
        "\n",
        "            loss = chamfer_distance(xyz_sampled, target_sampled)\n",
        "            acc = point_cloud_accuracy(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "            comp = point_cloud_completeness(xyz_sampled.detach().cpu().numpy(), target_sampled.detach().cpu().numpy())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "            total_comp += comp\n",
        "            progress.set_postfix(loss=loss.item(), acc=acc, comp=comp)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_acc = total_acc / len(train_loader)\n",
        "        avg_comp = total_comp / len(train_loader)\n",
        "        val_loss, val_acc, val_comp = evaluate(model, val_loader, device, desc=\"Validation\")\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f} | Comp: {avg_comp:.4f}\")\n",
        "        print(f\"               Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Comp: {val_comp:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/best_model.pth\")\n",
        "            print(f\"Saved new best model at epoch {epoch+1}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}. Best model from epoch {best_epoch}\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "oiD97FHGwFTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test and create folder\n",
        "def test(device):\n",
        "    test_dataset = MultiViewPointCloudDataset(\n",
        "        scenes_root=\"./7SCENES\",\n",
        "        pointcloud_root=\"./test_truth\",\n",
        "        scene_list = ['chess', 'fire', 'heads', 'office', 'pumpkin', 'redkitchen', 'stairs'],\n",
        "        kf_every=20,\n",
        "        views_per_sample=30,\n",
        "        size=384,\n",
        "        split='test'\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = RGBDPointPredictor().to(device)\n",
        "    model.load_state_dict(torch.load(\"./checkpoints/best_model.pth\"))\n",
        "    test_loss, test_acc, test_comp = evaluate(\n",
        "        model, test_loader, device,\n",
        "        desc=\"Test\",\n",
        "        save_results=True,\n",
        "        save_dir=\"./test\"\n",
        "    )\n",
        "\n",
        "    print(f\"[Test] Loss: {test_loss:.4f} | Acc: {test_acc:.4f} | Comp: {test_comp:.4f}\")"
      ],
      "metadata": {
        "id": "2m79mTrNruIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p test_truth\n",
        "!mkdir -p train_truth\n",
        "!mkdir -p test"
      ],
      "metadata": {
        "id": "frRoUy5Cp-mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scene_list = ['chess', 'fire', 'heads', 'office', 'pumpkin', 'redkitchen', 'stairs']\n",
        "generate_ground_truth_ply(\n",
        "    scenes_root=\"./7SCENES\",\n",
        "    pointcloud_root=\"./train_truth\",\n",
        "    scene_list=scene_list,\n",
        "    split=\"train\",\n",
        "    enable=True\n",
        ")\n",
        "\n",
        "# test split GT\n",
        "generate_ground_truth_ply(\n",
        "    scenes_root=\"./7SCENES\",\n",
        "    pointcloud_root=\"./test_truth\",\n",
        "    scene_list=scene_list,\n",
        "    split=\"test\",\n",
        "    enable=True\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prV4muxSqrWb",
        "outputId": "d87128a9-adfe-4915-b576-20634f7a776e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/60:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/60: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s, acc=0.284, comp=0.662, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss: 3.1453 | Acc: 0.4537 | Comp: 1.3342\n",
            "               Val   Loss: 0.7786 | Acc: 0.3110 | Comp: 0.5242\n",
            "Saved new best model at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/60: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s, acc=0.259, comp=0.343, loss=0.504]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Train Loss: 0.8330 | Acc: 0.3184 | Comp: 0.5058\n",
            "               Val   Loss: 0.3461 | Acc: 0.2910 | Comp: 0.2484\n",
            "Saved new best model at epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/60: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s, acc=0.189, comp=0.175, loss=0.173]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Train Loss: 0.3704 | Acc: 0.2749 | Comp: 0.2495\n",
            "               Val   Loss: 0.4578 | Acc: 0.2711 | Comp: 0.2881\n",
            "No improvement. Patience: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/60: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s, acc=0.143, comp=0.214, loss=0.244]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4] Train Loss: 0.2781 | Acc: 0.2441 | Comp: 0.2388\n",
            "               Val   Loss: 0.2495 | Acc: 0.1829 | Comp: 0.2129\n",
            "Saved new best model at epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/60: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s, acc=0.181, comp=0.233, loss=0.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 5] Train Loss: 0.2125 | Acc: 0.2078 | Comp: 0.2290\n",
            "               Val   Loss: 0.2842 | Acc: 0.1725 | Comp: 0.2385\n",
            "No improvement. Patience: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/60: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s, acc=0.217, comp=0.187, loss=0.172]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6] Train Loss: 0.1743 | Acc: 0.1871 | Comp: 0.2039\n",
            "               Val   Loss: 0.3049 | Acc: 0.1604 | Comp: 0.2782\n",
            "No improvement. Patience: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/60: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s, acc=0.414, comp=0.296, loss=0.383]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 7] Train Loss: 0.1588 | Acc: 0.1778 | Comp: 0.1981\n",
            "               Val   Loss: 0.2575 | Acc: 0.1614 | Comp: 0.1956\n",
            "No improvement. Patience: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/60: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s, acc=0.114, comp=0.21, loss=0.128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 8] Train Loss: 0.1361 | Acc: 0.1643 | Comp: 0.1956\n",
            "               Val   Loss: 0.2408 | Acc: 0.1571 | Comp: 0.2076\n",
            "Saved new best model at epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/60: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s, acc=0.544, comp=0.178, loss=0.387]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 9] Train Loss: 0.1273 | Acc: 0.1563 | Comp: 0.1857\n",
            "               Val   Loss: 0.2376 | Acc: 0.1530 | Comp: 0.2342\n",
            "Saved new best model at epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/60: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s, acc=0.115, comp=0.193, loss=0.106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 10] Train Loss: 0.1235 | Acc: 0.1566 | Comp: 0.1851\n",
            "               Val   Loss: 0.2386 | Acc: 0.1543 | Comp: 0.2226\n",
            "No improvement. Patience: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/60: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s, acc=0.0982, comp=0.185, loss=0.099]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 11] Train Loss: 0.1137 | Acc: 0.1479 | Comp: 0.1781\n",
            "               Val   Loss: 0.2769 | Acc: 0.1552 | Comp: 0.2588\n",
            "No improvement. Patience: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/60: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s, acc=0.131, comp=0.16, loss=0.108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 12] Train Loss: 0.1084 | Acc: 0.1455 | Comp: 0.1767\n",
            "               Val   Loss: 0.2772 | Acc: 0.1571 | Comp: 0.2374\n",
            "No improvement. Patience: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/60: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s, acc=0.103, comp=0.196, loss=0.0818]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 13] Train Loss: 0.1031 | Acc: 0.1411 | Comp: 0.1759\n",
            "               Val   Loss: 0.2453 | Acc: 0.1490 | Comp: 0.2254\n",
            "No improvement. Patience: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/60: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s, acc=0.116, comp=0.128, loss=0.0584]\n",
            "                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 14] Train Loss: 0.1004 | Acc: 0.1415 | Comp: 0.1704\n",
            "               Val   Loss: 0.2567 | Acc: 0.1509 | Comp: 0.2361\n",
            "No improvement. Patience: 5/5\n",
            "Early stopping at epoch 14. Best model from epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(device) #test and save"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcvxWfAyzdbN",
        "outputId": "c7a30b32-e4ec-43b9-f973-6a454fd58a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Test:   0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "                                                     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test] Loss: 0.8222 | Acc: 0.5178 | Comp: 0.1784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pziXnvaUu9DN"
      }
    }
  ]
}